@article{reimers2019sentence, 
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{artetxe2019massively,
  title={Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond},
  author={Artetxe, Mikel and Schwenk, Holger},
  journal={Transactions of the association for computational linguistics},
  volume={7},
  pages={597--610},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{ham2020kornli,
  title={KorNLI and KorSTS: New benchmark datasets for Korean natural language understanding},
  author={Ham, Jiyeon and Choe, Yo Joong and Park, Kyubyong and Choi, Ilji and Soh, Hyungjoon},
  journal={arXiv preprint arXiv:2004.03289},
  year={2020}
}
@article{reimers2020making,
  title={Making monolingual sentence embeddings multilingual using knowledge distillation},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2004.09813},
  year={2020}
}
@article{son2022translating,
  title={Translating Hanja historical documents to contemporary Korean and English},
  author={Son, Juhee and Jin, Jiho and Yoo, Haneul and Bak, JinYeong and Cho, Kyunghyun and Oh, Alice},
  journal={arXiv preprint arXiv:2205.10019},
  year={2022}
}
@inproceedings{yang2018applications,
  title={Applications research of machine learning algorithm in translation system},
  author={Yang, Lu and Chen, Da and Wu, Wenxue},
  booktitle={2018 6th International Conference on Machinery, Materials and Computing Technology (ICMMCT 2018)},
  pages={73--80},
  year={2018},
  organization={Atlantis Press}
}
@inproceedings{dyer2013simple,
  title={A simple, fast, and effective reparameterization of IBM model 2},
  author={Dyer, Chris and Chahuneau, Victor and Smith, Noah A},
  booktitle={Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics: human language technologies},
  pages={644--648},
  year={2013}
}
@article{cui2021pre,
  title={Pre-training with whole word masking for chinese bert},
  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3504--3514},
  year={2021},
  publisher={IEEE}
}
@article{popel2020transforming,
  title={Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals},
  author={Popel, Martin and Tomkova, Marketa and Tomek, Jakub and Kaiser, {\L}ukasz and Uszkoreit, Jakob and Bojar, Ond{\v{r}}ej and {\v{Z}}abokrtsk{\`y}, Zden{\v{e}}k},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{li2009chinese,
  title={Chinese syntactic reordering for adequate generation of Korean verbal phrases in Chinese-to-Korean SMT},
  author={Li, Jin-Ji and Kim, Jungi and Kim, Dong-Il and Lee, Jong-Hyeok},
  booktitle={Proceedings of the Fourth Workshop on Statistical Machine Translation},
  pages={190--196},
  year={2009}
}

% bleu score
@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@online{tedTalks,
  author = {Richard Saul Wurman, Harry Marks},
  title = {TED Talks},
  year = 1984,
  url = {https://www.ted.com/}
}
@online{NaverDictionary,
  author = {Lee Hae-jin},
  title = {Naver Dictionary},
  year = 1999,
  url = {https://dict.naver.com/}
}
% Googletrans
@online{googletrans,
  author = {SuHun Han},
  title = {Googletrans3.0.0},
  year = 2020,
  url = {https://dict.naver.com/}
}

% TED2020
@inproceedings{reimers-2020-multilingual-sentence-bert,
    title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2004.09813",
}

@online{ckip-bert,
  author = {Mu Yang},
  title = {ckip/bert-base-chinese},
  year = 2020,
  url = {https://huggingface.co/ckiplab/bert-base-chinese}
}
@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

%gpt-2
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

%cna dataset
@INPROCEEDINGS{10097977,
  author={Guo, Zhenliang and Huang, Zhen and Dou, Yong and Yu, Xiubin and Wang, Sijie and Chen, Zhongwu and Su, Xinxin and Liu, Xiaohang},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={CNA: A Dataset for Parsing Discourse Structure on Chinese News Articles}, 
  year={2022},
  volume={},
  number={},
  pages={990-995},
  keywords={Analytical models;Codes;Text categorization;Neural networks;Predictive models;Media;Artificial intelligence;discourse structure analysis;Chinese dataset;main event;multiple features},
  doi={10.1109/ICTAI56018.2022.00151}}

% kykim/bert-kor-base
@misc{kim2020lmkor,
  author = {Kiyoung Kim},
  title = {Pretrained Language Models For Korean},
  year = {2020},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/kiyoungkim1/LMkor}}
}

% konlpy
@inproceedings{park2014konlpy,
  title={KoNLPy: Korean natural language processing in Python},
  author={Park, Eunjeong L. and Cho, Sungzoon},
  booktitle={Proceedings of the 26th Annual Conference on Human & Cognitive Language Technology},
  address={Chuncheon, Korea},
  month={October},
  year={2014}
}

% dragon mapper
@online{dragonMapper,
  author = { Thomas Roten},
  title = {Dragon Mapper},
  year = 2014,
  url = {https://github.com/tsroten/dragonmapper}
}

@inproceedings{kiela-etal-2018-dynamic,
    title = "Dynamic Meta-Embeddings for Improved Sentence Representations",
    author = "Kiela, Douwe  and
      Wang, Changhan  and
      Cho, Kyunghyun",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1176",
    doi = "10.18653/v1/D18-1176",
    pages = "1466--1477",
    abstract = "While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems.",
}

% bert-nmt
@article{zhu2020incorporating,
  title={Incorporating bert into neural machine translation},
  author={Zhu, Jinhua and Xia, Yingce and Wu, Lijun and He, Di and Qin, Tao and Zhou, Wengang and Li, Houqiang and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2002.06823},
  year={2020}
}

% cross entropy loss
@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% pca
@article{MACKIEWICZ1993303,
title = {Principal components analysis (PCA)},
journal = {Computers & Geosciences},
volume = {19},
number = {3},
pages = {303-342},
year = {1993},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(93)90090-R},
url = {https://www.sciencedirect.com/science/article/pii/009830049390090R},
author = {Andrzej Maćkiewicz and Waldemar Ratajczak}}
%keywords = {Principal Components Analysis, Variance-covariance matrix, Coefficients of determination, Eigenvalues, Eigenvectors, Correlation matrix, Bartlett's statistics, FORTRAN 77},
%abstract = {Principal Components Analysis (PCA) as a method of multivariate statistics was created before the Second World War. However, the wider application of this method only occurred in the 1960s, during the “Quantitative Revolution” in the Natural and Social Sciences. The main reason for this time-lag was the huge difficulty posed by calculations involving this method. Only with the advent and development of computers did the almost unlimited application of multivariate statistical methods, including principal components, become possible. At the same time, requirements arose for precise numerical methods concerning, among other things, the calculation of eigenvalues and eigenvectors, because the application of principal components to technical problems required absolute accuracy. On the other hand, numerous applications in Social Sciences gave rise to a significant increase in the ability to interpret these nonobservable variables, which is just what the principal components are. In the application of principal components, the problem is not only to do with their formal properties but above all, their empirical origins. The authors considered these two tendencies during the creation of the program for principal components. This program—entitled PCA—accompanies this paper. It analyzes consecutively, matrices of variance-covariance and correlations, and performs the following functions: •- the determination of eigenvalues and eigenvectors of these matrices.•- the testing of principal components.•- the calculation of coefficients of determination between selected components and the initial variables, and the testing of these coefficients,•- the determination of the share of variation of all the initial variables in the variation of particular components,•- construction of a dendrite for the initial set of variables,•- the construction of a dendrite for a selected pattern of the principal components,•- the scatter of the objects studied in a selected coordinate system. Thus, the PCA program performs many more functions especially in testing and graphics, than PCA programs in conventional statistical packages. Included in this paper are a theoretical description of principal components, the basic rules for their interpretation and also statistical testing.}
}

% tsne
% JMLR:v9:vandermaaten08a,
@article{ tsne,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@inproceedings{sun-etal-2021-chinesebert,
    title = "{C}hinese{BERT}: {C}hinese Pretraining Enhanced by Glyph and {P}inyin Information",
    author = "Sun, Zijun  and
      Li, Xiaoya  and
      Sun, Xiaofei  and
      Meng, Yuxian  and
      Ao, Xiang  and
      He, Qing  and
      Wu, Fei  and
      Li, Jiwei",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.161",
    doi = "10.18653/v1/2021.acl-long.161",
    pages = "2065--2075",
    abstract = "Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the \textit{glyph} and \textit{pinyin} information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.",
}